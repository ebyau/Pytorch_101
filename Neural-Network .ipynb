{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from torch.optim import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs #---Builds a synthetic dataset of example data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (hidden_layer_1): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (activation_1): ReLU()\n",
      "  (output_layer): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# construction a shallow sequential neural network\n",
    "\n",
    "\n",
    "def get_model(inFeatures=4, hiddenDim=8, nbClasses=3):\n",
    "    mlpModel = nn.Sequential(OrderedDict([\n",
    "        ('hidden_layer_1',nn.Linear(inFeatures,hiddenDim)),\n",
    "        ('activation_1',nn.ReLU()),\n",
    "        ('output_layer',nn.Linear(hiddenDim,nbClasses))\n",
    "    ]))\n",
    "    \n",
    "    return mlpModel\n",
    "mlp = get_model()\n",
    "print(mlp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(inputs,targets,batchSize):\n",
    "    \n",
    "#     ```\n",
    "#     inputs: Our input data to the neural network\n",
    "#     targets: Our target output values (i.e., what we want our neural network to accurately predict)\n",
    "#     batchSize: Size of data batch\n",
    "#     ```\n",
    "    # loop over the datset in batchsize chunks\n",
    "    for i in range(0,inputs.shape[0],batchSize):\n",
    "        #yeild tuple of current batched data and labels\n",
    "        yield(inputs[i:i + batchSize],targets[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training using cpu....\n"
     ]
    }
   ],
   "source": [
    "# HyPERPARAMETERS\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS = 30\n",
    "LR = 1e-2\n",
    "\n",
    "#determine the training device\n",
    "DEVICE = 'Cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"[INFO] training using {}....\".format(DEVICE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing data....\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] preparing data....\")\n",
    "# generate a 3-class classification problem with 1000 data points,\n",
    "(X,y) = make_blobs(n_samples=1000,n_features=4,centers=3,\n",
    "                  cluster_std=2.5,random_state=95)\n",
    "\n",
    "(trainX,testX,trainY,testY) = train_test_split(X,y,\n",
    "                    test_size=0.15,random_state=95)\n",
    "\n",
    "# the training and testing data is converted to PyTorch tensors from NumPy arrays\n",
    "trainX = torch.from_numpy(trainX).float()\n",
    "trainY = torch.from_numpy(trainY).float()\n",
    "testY = torch.from_numpy(testY).float()\n",
    "testX = torch.from_numpy(testX).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize our model \n",
    "mlp = mlp.to(DEVICE)\n",
    "\n",
    "# loss function and optimizer\n",
    "opt = SGD(mlp.parameters(),lr=LR)\n",
    "lossFunc = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch: 1...\n",
      "epoch: 1 train loss: 1.265 train accuracy: 0.327\n",
      "epoch: 1 test loss: 0.863 test accuracy: 0.573\n",
      "\n",
      "[INFO] epoch: 2...\n",
      "epoch: 2 train loss: 0.801 train accuracy: 0.638\n",
      "epoch: 2 test loss: 0.622 test accuracy: 0.720\n",
      "\n",
      "[INFO] epoch: 3...\n",
      "epoch: 3 train loss: 0.616 train accuracy: 0.680\n",
      "epoch: 3 test loss: 0.505 test accuracy: 0.740\n",
      "\n",
      "[INFO] epoch: 4...\n",
      "epoch: 4 train loss: 0.516 train accuracy: 0.754\n",
      "epoch: 4 test loss: 0.434 test accuracy: 0.813\n",
      "\n",
      "[INFO] epoch: 5...\n",
      "epoch: 5 train loss: 0.449 train accuracy: 0.812\n",
      "epoch: 5 test loss: 0.382 test accuracy: 0.853\n",
      "\n",
      "[INFO] epoch: 6...\n",
      "epoch: 6 train loss: 0.395 train accuracy: 0.865\n",
      "epoch: 6 test loss: 0.338 test accuracy: 0.880\n",
      "\n",
      "[INFO] epoch: 7...\n",
      "epoch: 7 train loss: 0.346 train accuracy: 0.895\n",
      "epoch: 7 test loss: 0.297 test accuracy: 0.907\n",
      "\n",
      "[INFO] epoch: 8...\n",
      "epoch: 8 train loss: 0.301 train accuracy: 0.920\n",
      "epoch: 8 test loss: 0.257 test accuracy: 0.927\n",
      "\n",
      "[INFO] epoch: 9...\n",
      "epoch: 9 train loss: 0.260 train accuracy: 0.939\n",
      "epoch: 9 test loss: 0.221 test accuracy: 0.953\n",
      "\n",
      "[INFO] epoch: 10...\n",
      "epoch: 10 train loss: 0.222 train accuracy: 0.956\n",
      "epoch: 10 test loss: 0.190 test accuracy: 0.980\n",
      "\n",
      "[INFO] epoch: 11...\n",
      "epoch: 11 train loss: 0.192 train accuracy: 0.964\n",
      "epoch: 11 test loss: 0.165 test accuracy: 0.987\n",
      "\n",
      "[INFO] epoch: 12...\n",
      "epoch: 12 train loss: 0.167 train accuracy: 0.973\n",
      "epoch: 12 test loss: 0.145 test accuracy: 0.987\n",
      "\n",
      "[INFO] epoch: 13...\n",
      "epoch: 13 train loss: 0.148 train accuracy: 0.975\n",
      "epoch: 13 test loss: 0.129 test accuracy: 0.987\n",
      "\n",
      "[INFO] epoch: 14...\n",
      "epoch: 14 train loss: 0.133 train accuracy: 0.978\n",
      "epoch: 14 test loss: 0.116 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 15...\n",
      "epoch: 15 train loss: 0.120 train accuracy: 0.981\n",
      "epoch: 15 test loss: 0.105 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 16...\n",
      "epoch: 16 train loss: 0.110 train accuracy: 0.984\n",
      "epoch: 16 test loss: 0.097 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 17...\n",
      "epoch: 17 train loss: 0.102 train accuracy: 0.984\n",
      "epoch: 17 test loss: 0.090 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 18...\n",
      "epoch: 18 train loss: 0.095 train accuracy: 0.985\n",
      "epoch: 18 test loss: 0.084 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 19...\n",
      "epoch: 19 train loss: 0.089 train accuracy: 0.985\n",
      "epoch: 19 test loss: 0.080 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 20...\n",
      "epoch: 20 train loss: 0.084 train accuracy: 0.986\n",
      "epoch: 20 test loss: 0.076 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 21...\n",
      "epoch: 21 train loss: 0.079 train accuracy: 0.987\n",
      "epoch: 21 test loss: 0.072 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 22...\n",
      "epoch: 22 train loss: 0.076 train accuracy: 0.988\n",
      "epoch: 22 test loss: 0.069 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 23...\n",
      "epoch: 23 train loss: 0.072 train accuracy: 0.989\n",
      "epoch: 23 test loss: 0.067 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 24...\n",
      "epoch: 24 train loss: 0.070 train accuracy: 0.989\n",
      "epoch: 24 test loss: 0.065 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 25...\n",
      "epoch: 25 train loss: 0.067 train accuracy: 0.989\n",
      "epoch: 25 test loss: 0.063 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 26...\n",
      "epoch: 26 train loss: 0.065 train accuracy: 0.989\n",
      "epoch: 26 test loss: 0.061 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 27...\n",
      "epoch: 27 train loss: 0.063 train accuracy: 0.991\n",
      "epoch: 27 test loss: 0.060 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 28...\n",
      "epoch: 28 train loss: 0.061 train accuracy: 0.991\n",
      "epoch: 28 test loss: 0.058 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 29...\n",
      "epoch: 29 train loss: 0.059 train accuracy: 0.991\n",
      "epoch: 29 test loss: 0.057 test accuracy: 0.993\n",
      "\n",
      "[INFO] epoch: 30...\n",
      "epoch: 30 train loss: 0.058 train accuracy: 0.991\n",
      "epoch: 30 test loss: 0.056 test accuracy: 0.993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainTemplate = \"epoch: {} test loss: {.3f} test accuracy: {.3f}\"\n",
    "\n",
    "#loop through the epochs\n",
    "for epoch in range(0,EPOCHS):\n",
    "    # initialize tracker variables and set our model to trainable\n",
    "    print(\"[INFO] epoch: {}...\".format(epoch+1))\n",
    "    trainLoss = 0\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    mlp.train()\n",
    "    # loop over the current data batch\n",
    "    for (batchX,batchY) in next_batch(trainX,trainY,BATCH_SIZE):\n",
    "        \n",
    "        # move current batch to DEVICE, run through model calc loss\n",
    "        (batchX,batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))\n",
    "        predictions = mlp(batchX)\n",
    "        loss = lossFunc(predictions, batchY.long())\n",
    "        \n",
    "        #set gradients accumulated from prev step to zero\n",
    "        #perform back propagation and update model params\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        #update train loss, accuracy, and num of samples visited\n",
    "        trainLoss += loss.item() * batchY.size(0)\n",
    "        trainAcc +=  (predictions.max(1)[1] == batchY).sum().item()\n",
    "        samples += batchY.size(0)\n",
    "        \n",
    "        \n",
    "    trainTemplate = \"epoch: {} train loss: {:.3f} train accuracy: {:.3f}\"\n",
    "    print(trainTemplate.format(epoch+1,(trainLoss/samples),(trainAcc /samples)))\n",
    "    \n",
    "    # initialize tracker variables\n",
    "    testLoss = 0\n",
    "    testAcc = 0\n",
    "    samples = 0\n",
    "    mlp.eval()\n",
    "\n",
    "    # intialize with no gradients\n",
    "    with torch.no_grad():\n",
    "        for (batchX,batchY) in next_batch(testX,testY,BATCH_SIZE):\n",
    "\n",
    "            # move current batch to DEVICE, run through model calc loss\n",
    "            (batchX,batchY) = (batchX.to(DEVICE), batchY.to(DEVICE))\n",
    "            predictions = mlp(batchX)\n",
    "            loss = lossFunc(predictions, batchY.long())\n",
    "\n",
    "\n",
    "            #update train loss, accuracy, and num of samples visited\n",
    "            testLoss += loss.item() * batchY.size(0)\n",
    "            testAcc +=  (predictions.max(1)[1] == batchY).sum().item()\n",
    "            samples += batchY.size(0)\n",
    "\n",
    "        testTemplate = \"epoch: {} test loss: {:.3f} test accuracy: {:.3f}\"\n",
    "        print(testTemplate.format(epoch+1,(testLoss/samples),(testAcc /samples)))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "dl4cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
